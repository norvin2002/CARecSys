---
title: "CA - Recommender System 2019"
author: "Norvin Chandra - E0384966"
date: "28 Oct 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
```

## Project Objective


```{r, message = FALSE, echo = FALSE}
pacman::p_load(tidyverse, tm, stringr,NLP, RColorBrewer, wordcloud, Matrix, recommenderlab, slam, data.table, rARPACK, recosystem)

setwd('C:/Users/nchandra/OneDrive - National University of Singapore/CARecSys')


#memory.limit()

df <- fread("./Data/amazon/Reviews.csv", sep = ",", header= TRUE)
```

## Data Exploration and Preparation
Get the Amazon Review Data
```{r, echo = FALSE}
kable(tail(df, 2), format = "html")
```

Data Structure
```{r, echo = FALSE}
str(df)
```

```{r, echo = FALSE}
reviews <- as.data.frame(df)
```


```{r}
# Delete irrelevant character from UserId
reviews$UserId <- str_replace_all(reviews$UserId, '#oc-', '' )

# remove duplicated UsedId and ProductId
reviews <- reviews[!duplicated(reviews[,c(2,3)]),]

# Select needed columns, ProductId, Id, UserId, ProfileName, Score, Summary, Text, count, mean_score
reviews2 <- reviews[,c(1:4, 7, 9:10)]

# combine Summary and the review Text body
reviews2 <- reviews2 %>%
    mutate(combine_summary = paste(Summary, Text))
```
Check for Duplicated combine_summary and remove

```{r}
nrow(reviews2[duplicated(reviews2$combine_summary),])

reviews2 <- reviews2[!duplicated(reviews2$combine_summary),]
```

```{r}
#head(reviews2)
```

We counted number of review for each product to find out product with low review to be filtered out to reduce computational time. 

```{r, echo = FALSE}
# get the count of review for each ProductId, use to filter out products with low review count
p_count <- reviews2 %>%
    group_by(ProductId) %>%
    summarize(count = n()) %>%
    arrange(-count)

#p_count

reviews2 <- merge(reviews2, p_count, by.x = 'ProductId', by.y = 'ProductId', all.x = T)

kable(head(reviews2), format = "html")
```

Median count of Review
```{r}
#median count of product
median_count <- median(reviews2$count)
median_count

#only keep product with more than median reviews
review_final <- reviews2 %>%
    filter(count >= median_count)
    
dim(review_final)
```

Most Reviewed Products are:
```{r, echo = FALSE}
# Most reviewed product
review_final %>%
    group_by(ProductId) %>%
    summarize(count = n()) %>%
    arrange(-count) %>%
    head()
```

Number of unique user and product after removal of low review count products
```{r}
# Unique user
print(paste('Number of unique user:', length(unique(review_final$UserId))))

# Unique product
print(paste('Number of unique product:', length(unique(review_final$ProductId))))
```

In view of our computing resource limitation, we have decided to work on the first 50k of the review data.
```{r}
# subset to 1st 50k data only for this project
review_final_small <- review_final[1:50000,]
```

```{r, echo = FALSE}
# remove earlier count done at full dataset
review_final_small$count = NULL
```

After subsetting the data, we redo the count of review for each product and also the average rating given
```{r, echo = FALSE}
# Re run review Count for each product in the subset only
p_count <- review_final_small %>%
    group_by(ProductId) %>%
    summarize(count = n()) %>%
    arrange(-count)

# Mean score for each product
mean_score <- review_final_small %>%
    group_by(ProductId) %>%
    summarize(mean_score = mean(Score))

# merge p_count to reviews2
review_final_small <- merge(review_final_small, p_count, by.x = 'ProductId', by.y = 'ProductId', all.x = TRUE)

# merge mean_score to reviews2
reviews_final_small <- merge(review_final_small, mean_score, by.x = 'ProductId', by.y = 'ProductId', all.x = TRUE)

kable(head(review_final_small, 1), format = "html")
```


```{r, echo = FALSE}
# double check for duplicate review text
nrow(review_final_small[duplicated(review_final_small$combine_summary),])

p_count <- review_final_small %>%
    group_by(ProductId) %>%
    summarize(count = n()) %>%
    arrange(-count)
```

Check minimum number of review and maximum number of review for the products
```{r}
# check if products are having enough review
min(review_final_small$count)

max(review_final_small$count)

dim(review_final_small)
```

As the data consists of user rating and user review in text format, we are able to build recommender system using Collaborative Filtering approach and also Content Based approach.

## Collaborative Filtering

### Data Preparation specific for Collaborative Filtering
For Collaborative Filtering (CF) we only need the UserId, ProductId and Rating
```{r}
# select only ProductId, UserId, Score
review_cf <- review_final_small[,c(3,1,5)]

summary(review_cf$Score)
```

Basic exploration of the rating
```{r}
hist(review_cf$Score)

boxplot(review_cf$Score)
```

As confirmed by the histogram and boxplot, the rating is positively skewed with 5 as majority on the scale of 1 - 5. 

Quick Look at the data for CF model
```{r, echo = FALSE}
kable(head(review_cf), format = "html")
```

```{r, echo = FALSE, message = FALSE}
print(paste('Number of user:', length(unique(review_final_small$UserId))))
print(paste('Number of product:',length(unique(review_final_small$ProductId))))
```

```{r, echo = FALSE, eval = FALSE, message = FALSE}
review_final_small %>%
    group_by(UserId) %>%
    summarize(count = n()) %>%
    arrange(-count) %>%
    tail()
```
As the number of user exceeds number of product, Item Based Collaborative Filtering (IBCF) is adopted.
Another reason is due to the similarity among products tend to change at much slower pace as compared to the similarity among users

## IBCF - using recommenderlab package

Format the data to appropriate format to create sparse matrix
```{r, echo = FALSE}
# set data format
review_cf$UserId <- as.factor(review_cf$UserId)
review_cf$ProductId <- as.factor(review_cf$ProductId)
review_cf$Score <- as.numeric(review_cf$Score)
```

```{r, echo = FALSE, eval = FALSE}
############################# code below not in use############################################

#review_matrix <- dcast(review_cf, ProductId ~ UserId, fun.aggregate = mean, value.var = 'Score', fill = 0)

#review_matrix[1:10, 30:60]

#rownames(review_matrix) = review_matrix[,1]

#review_matrix[,1] = NULL

#print(object.size(review_matrix), units = 'auto')

#dim(review_matrix)

#review_matrix[1:5,1:5]

###############################################################################################
```

```{r}
# Create sparse matrix
review_spmatrix <- sparseMatrix(i = as.integer(review_cf$UserId),
                               j = as.integer(review_cf$ProductId),
                               x = as.numeric(review_cf$Score),
                               dimnames = list(unique(review_cf$UserId), 
                                               unique(review_cf$ProductId))
                                )

```

```{r}
dim(review_spmatrix)
```

```{r, eval = FALSE}
review_spmatrix[1:10,1:5]
```

Sparse matrix has much smaller size
```{r}
print(object.size(review_spmatrix), units = 'auto')
```

In order to use recommenderlab, we need to convert sparse matrix to realRatingMatrix
```{r}
#format sparse matrix for recommenderlab library
review_sprrm <- as(review_spmatrix, 'realRatingMatrix')

review_sprrm

dim(review_sprrm)

print(object.size(review_sprrm), units = 'auto')
```

Split the data into training and testing subsets on 80/20
```{r}
given <- 1 #refer to https://stackoverflow.com/questions/30128383/error-evaluationscheme-recommenderlab-in-r
# some of the UserId only has 1 review so the given value cannot be more than 1
rating_threshold <- 4 # set 4 and above considered positive
eval <- evaluationScheme(review_sprrm, method = 'split', train = 0.8, given = given, goodRating = rating_threshold)
```

We will run a few variations of IBCF and also try Popular model to compare the result

1. IBCF with cosine similarity and not normalized data
```{r}
#IBCF with cosine similarity and not normalized data for k most similar items
IBCF_N_C <- Recommender(data = getData(eval, 'train'),
                        method = "IBCF", 
                        parameter = list(normalize = NULL, 
                                         method = 'Cosine', k = 30))
```

2. IBCF with cosine similarity and centered data
```{r}
# IBCF with cosine similarity and centered data for k most similar items
IBCF_C_C <- Recommender(data = getData(eval, 'train'), 
                        method = 'IBCF', 
                        parameter = list(normalize = "center", method = "Cosine", k = 30))
```

3. IBCF with cosine similarity and z-score normalized data
```{r}
# IBCF with cosine similarity and z-score normalised data for k most similar item
IBCF_Z_C <- Recommender(data = getData(eval, 'train'),
                       method = 'IBCF',
                       parameter = list(normalize = "Z-score", method = 'Cosine', k = 30))
```

4. Popular Model with centered data
```{r}
# Popular Model
POP <- Recommender(data = getData(eval, 'train'), method = 'POPULAR', param = list(normalize = 'center'))
```

We then evaluate the model against test data
```{r}
# Evaluation of IBCF
pIBCF_N_C <- predict(IBCF_N_C, getData(eval, 'known'), type = 'ratings')
pIBCF_C_C <- predict(IBCF_C_C, getData(eval, 'known'), type = 'ratings')
pIBCF_Z_C <- predict(IBCF_Z_C, getData(eval, 'known'), type = 'ratings')
pPOP <- predict(POP, getData(eval, 'known'), type = 'ratings')
```

The performance statistics below shows that IBCF with cosine similarity on not normalized data performs best
```{r}
# Performance statistics
perf_stats <- rbind(
    IBCF_N_C = calcPredictionAccuracy(pIBCF_N_C, getData(eval, 'unknown')),
    IBCF_C_C = calcPredictionAccuracy(pIBCF_C_C, getData(eval, 'unknown')),
    IBCF_Z_C = calcPredictionAccuracy(pIBCF_Z_C, getData(eval, 'unknown')),
    pPOP = calcPredictionAccuracy(pPOP, getData(eval, 'unknown'))
    )

perf_stats

#rm(IBCF_N_C, IBCF_C_C, IBCF_Z_C)
```

```{r}
IBCF_N_C
```

Test IBCF_N_C on unknown data
```{r, echo = FALSE, eval = FALSE}
# predict items for user
pred<-predict(IBCF_N_C, getData(eval,'unknown'), type = 'ratings')

# item - item cosine similarity matrix
as(pred, 'matrix')[100:150, 200:250]
```

```{r}
pred<-predict(IBCF_N_C, getData(eval,'unknown'), n= 5)

as(pred, "list")[1:5]
```

We use the model to try recommend product to 1 existing user whose data is in the matrix
```{r}
# try to recommend item to 1 specific user
review_cf[review_final_small$UserId %in% c('AYZ0PR5QZROD1'),]
```
```{r}
rec_item <- predict(IBCF_N_C, review_sprrm["AYZ0PR5QZROD1",], n= 5 )

# display the recommendation
as(rec_item, "list")
```
Get the ratings of the recommended items
```{r}
getRatings(rec_item)
```

## Matrix Factorisation Approach
### ALTERNATING LEAST SQUARE on RECOSYSTEM

```{r}
# Train test split for Recosystem ALS
smp_size <- floor(0.9 * nrow(review_cf))
train_indexes <- sample(1: nrow(review_cf), size = smp_size)
trainevents <- review_cf[train_indexes, ]; dim(trainevents)
testevents  <- review_cf[-train_indexes, ]; dim(testevents)

#testevents1  <- review_cf[-train_indexes, ]; dim(testevents)
#head(testevents1)
```

Create Train and Test set in recosystem format
```{r}
# load into recosystem format
trainset = data_memory(trainevents$UserId, trainevents$ProductId, trainevents$Score, index1= TRUE)
testset  = data_memory(testevents$UserId, testevents$ProductId, testevents$Score, index1= TRUE)
```

```{r}
# get optimised factorisation using r$tune
r = Reco()
opts = r$tune(trainset, opts=list(dim=c(20, 30, 40), lrate=c(0.1,0.2), costp_l1=0, costq_l1=0, niter=40))

#opts

opts$min
```
The min result shows dimension of 20. There is a possibility that the minimum dim is below 20 but shown as 20 as our lowest dim parameter set in initial run is 20. We will run again to try dimension 5, 10 and 20

```{r}
opts = r$tune(trainset, opts=list(dim=c(5, 10, 20), lrate=c(0.1,0.2), costp_l1=0, costq_l1=0, niter=40))

#opts

opts$min
```
This time, the minimum dimension is 5. Again because 5 is the lowest in our list of test dimension, we will run the tune command one more time with narrower dimensions to ascertain the minimum dimension

```{r}
opts = r$tune(trainset, opts=list(dim=c(1:5), lrate=c(0.1,0.2), costp_l1=0, costq_l1=0, niter=40))

#opts

opts$min
```

Best opts obtained and we will proceed to train using the best opts
```{r}
r$train(trainset, opts = opts$min)
```

Get the predicted result
```{r}
# get predictions:  this multiplies the user vectors in testset, with the item vectors in Q
testevents$prediction <- r$predict(testset, out_memory()) 
head(testevents)
```

We subset data from the main dataset as a test data for ALS model
```{r}
# New dataset not seen before purely for testing ALS model
review_cftest <- review_final[100001:100500,c(3,1,5)]

review_cftest$UserId <- as.factor(review_cftest$UserId)
review_cftest$ProductId <- as.factor(review_cftest$ProductId)
review_cftest$Score <- as.numeric(review_cftest$Score)

#head(review_cftest)

# convert the test data to recosystem format
review_cftestset  = data_memory(review_cftest$UserId, review_cftest$ProductId, review_cftest$Score, index1= TRUE)

review_cftest$prediction <- r$predict(review_cftestset, out_memory())

kable(head(review_cftest, 30), format = "html")
```
The predicted Rating looks close to actual Rating.

So far the test is done by inputing data in same format of actual train data, consisting of UserId, ProductId. For the recommender system to function, we will need it to take input of a few product the customer likes or have purchased and generate a list of recommended products.
We have obtained the user latent feature matrix and product latent feature matrix which we can use to predict products to be recommended.

## Matrix Factorisation Manual Calculation Approach to recommend products to user given the product info provided by user
```{r}
#get factorised matrices
r$output() # exports the two matrix to the current directory (as mat_P.txt, mat_Q.txt)

P = as.matrix(read.table("mat_P.txt")) # user
Q = as.matrix(read.table("mat_Q.txt")) # product
```
We assign UserId and ProductId as index to the matrices
```{r}
rownames(P) = as.factor(unique(review_cf$UserId))
rownames(Q) = as.factor(unique(review_cf$ProductId))
head(P) # the user factors matrix, rows = user, columns are the latent features
head(Q) # the item factors matrix, rows = items, columns are the latent features
```

### Testing the ALS Recommender ###
```{r}
# Create a string containing 5 new products through randomly selecting 5 products from the ProductId list
cust_order <- sample(unique(review_cf$ProductId), size = 5)

#drop levels and convert to string
cust_order <- levels(droplevels(cust_order))

cust_order
```

The following codes take in the imaginary cust_order and for each of the item, it does a matrix multiplication to find top 5 users based on rating. Thereafter, for each of the 5 users, find top 5 products with highest rating to be returned as product to be recommended to customer placing the imaginary cust_order
```{r}
# empty matrix container for the final result
result = matrix(nrow = 0, ncol = 2)

#for each item in the cust_order, do a matrix multiplication to find top 5 user based on rating
for (item in cust_order){
    T = as.matrix(Q[item,])
    prats = P %*% T
    prats= prats[order(prats, decreasing = TRUE),]
    similar_user = names(prats)[1:5]
    #print(similar_user)
    
    # for each user, do a matrix multiplication back to find relevant products
    for (name in similar_user){
        # empty temporary matrix
        mat = ''
        T2 = as.matrix(P[name,])
        prats2 = Q %*% T2
        prats2 = prats2[order(prats2, decreasing = TRUE),]
        similar_product = prats2[1:5]
        mat <- cbind(as.vector(names(similar_product)), as.vector(similar_product))
        #print(mat)
        #retutn a matrix
        result <- rbind(result, mat)
    }
    
}
```

We then format the result, group them by ProductId to obtain average Rating as the rating for each product differ from one UserId to another UserId.
```{r}
#name the matrix
colnames(result) <- c('ProductId', 'Rating')

#dim(result)

# convert to dataframe
result_df <- as.data.frame(result[order(result[,2], result[,1], decreasing = TRUE),])

# convert Rating from factor to numeric, extra step in between, convert to char before numeric
result_df$Rating <- as.numeric(as.character(result_df$Rating))

#head(result_df,10)

# groupby and get the mean rating for each product, sort descending
cf_recom_product <- result_df %>%
    group_by(ProductId) %>%
    summarize(meanRating = mean(Rating)) %>%
    arrange(-meanRating) %>%
    mutate(Rank = order(meanRating, decreasing = TRUE))

# top 5 relevant products
cf_recom_product
```

```{r, echo = FALSE, eval = FALSE}
#################### CODE BLOCK BUILDING ###############

# Create a string containing 6 new products
cust_order1 <- sample(review_cf$ProductId, size = 1)

cust_order1 <- levels(droplevels(cust_order1))

target = cust_order1

T = as.matrix(Q[target,]) ; T

head(P)

prats = P %*% (T)

head(prats)

prats= prats[order(prats, decreasing = TRUE),]

head(prats)

#top 1 similar user
similar_user <- names(prats)[1]

similar_user

T2 = as.matrix(P[similar_user,]) ; T2

prats2 = Q %*% (T2)

prats2 = prats2[order(prats2[,1], decreasing = TRUE),]

# top 5 recommended products
prats2[1:5]

########################### END OF CODE BLOCK #########################
```

```{r, echo = FALSE}
#clear object from RAM
rm(opts)
rm(IBCF_C_C)
rm(IBCF_Z_C)
rm(POP)
rm(reviews)
```

## Content Based Recommender
Our second approach is to make use of the review text to obtain cosine similarity among products. With the similarity score, we then will be able to recommend items closest to the test items presented.

```{r}

# data for content based recommender
review_cb <- review_final_small
```

```{r, eval = FALSE, echo = FALSE}
#review_cb <- ungroup(review_final_small) %>%
#    group_by(ProductId) %>%
#    mutate(combine_summary2 = paste(Summary, collapse= ' '))

# check for duplication
review_cb[duplicated(review_cb$combine_summary),]

review_cb[review_cb$combine_summary == "very good This product is a very health snack for your pup as it is made of 100% beef liver. My puppy does all of his tricks to get this treat. It is a little pricy but the container is large so it should last a long time as long as you don't overfeed.",]
```

```{r, echo = FALSE}
#sort by ProductId ascending
review_cb <- review_cb[order(review_cb$ProductId), ]

review_cb_prod <- review_cb[,c(1,8,9)]

review_cb_prod <- review_cb_prod[order(review_cb_prod$ProductId),]
```

We combine all the review text and summary for each product.

```{r}
#collapse the summary to each ProductId
review_cb_prod2 <- aggregate(combine_summary ~ ProductId, data = review_cb_prod, FUN = paste, collapse = " ")

dim(review_cb_prod2)
```

1266 Products

```{r, eval = FALSE}
#check for duplication
nrow(review_cb_prod2[duplicated(review_cb_prod2$combine_summary),])
```

```{r, eval = FALSE}
# check lengths
length(unique(review_cb_prod2$combine_summary))
length(unique(review_cb_prod2$ProductId))
```

We proceed with processing the text to clean up unwanted text and convert them into Document Term Matrix

```{r}
# Remove non ASCII Character
review_cb_prod2$combine_summary <- iconv(review_cb_prod2$combine_summary, "UTF-8", "ASCII",sub='')

mystopwords = c(stopwords('english'), 'the', 'and', 'will', 'product')

desc_corpus <- VCorpus(VectorSource(review_cb_prod2$combine_summary))
desc_corpus <- tm_map(desc_corpus, content_transformer(tolower))
desc_corpus <- tm_map(desc_corpus, removeNumbers)
desc_corpus <- tm_map(desc_corpus, removeWords, mystopwords)
desc_corpus <- tm_map(desc_corpus, removePunctuation)
desc_corpus <- tm_map(desc_corpus, stemDocument)
desc_corpus <- tm_map(desc_corpus, removeWords, mystopwords)
desc_corpus <- tm_map(desc_corpus, stripWhitespace)
```

```{r, eval = FALSE}
for(i in 1:3){
    print(desc_corpus[[i]][1])
}
```

Create TFIDF DTM
```{r}
#creating the Document Term Matrix
dtm <- DocumentTermMatrix(desc_corpus)
dtm_ti <- weightTfIdf(dtm)
dtm_ti
```

The DTM is quite sparse. We try to reduce the sparsity
```{r}
dtm_ti2 = removeSparseTerms(dtm_ti, 0.99)

dtm_ti2
```

Some of the words in the DTM
```{r}
freq = data.frame(sort(colSums(as.matrix(dtm_ti2)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
```

```{r, echo = FALSE}
gc()
```

After the DTM is in order, we will use slam package to compute cosine similarity among products
```{r}
## Compute cosine similarity
# slam package for cosine similarity computation
sim_mat_cos <- crossprod_simple_triplet_matrix(t(dtm_ti2))/(sqrt(col_sums(t(dtm_ti2)^2) %*% t(col_sums(t(dtm_ti2)^2))))
```

Check if the dimension of sim_mat_cos is the same as number of unique product
```{r}
length(unique(review_cb_prod2$ProductId)) == dim(sim_mat_cos)[1]
```

Assign colnames and rownames to the sim_mat_cos for indexing purpose
```{r}
rownames(sim_mat_cos) <- unique(review_cb_prod2$ProductId)
colnames(sim_mat_cos) <- unique(review_cb_prod2$ProductId)
#sim_mat_cos[1:5,1:30]
```

```{r, eval = FALSE}
## Function to return a prediction matrix, cannot get output out, need some working ###
#recProd = function (simm, prod, k) {
#    recom = matrix(ncol =2, nrow = 0)
#    for (i in cust_order){
#        output = ''
#        result <- sort(simm[, i], decreasing = TRUE)[2:(k+1)]
#        output <- cbind(as.vector(names(result)), as.vector((result)))
#        recom <- rbind(recom, output)
#        }
#    #print(recom)
#    return(recom)
#    }

#sort(sim_mat_cos[, 'B00004RBDZ'], decreasing = TRUE)[2:(5+1)]
```

For each of product input, the code below will generate k number of product to be recommended and combined into a dataframe

```{r}
k = 5 # number of product to be recommended
simm = sim_mat_cos 
#create empty matrix to hold the output
recom = matrix(ncol =2, nrow = 0)
for (i in cust_order){
    output = ''
    result <- sort(simm[, i], decreasing = TRUE)[2:(k+1)]
    output <- cbind(as.vector(names(result)), as.vector((result)))
    recom <- rbind(recom, output)
    }

colnames(recom) <- c('ProductId', 'Rating')

cb_recom <- as.data.frame(recom[order(recom[,2], recom[,1], decreasing = TRUE),])

# convert Rating from factor to numeric, extra step in between, convert to char before numeric
cb_recom$Rating <- as.numeric(as.character(cb_recom$Rating))

kable(head(cb_recom,10), format = "html")
```

We combine multiple rating for same product and take the average rating before sorting them descending
```{r}
# groupby and get the mean rating of each product, sort descending
cb_recom_product <- cb_recom %>%
    group_by(ProductId) %>%
    summarize(meanRating = mean(Rating)) %>%
    arrange(-meanRating) %>%
    mutate(Rank = order(meanRating, decreasing = TRUE))
```

```{r}
# all relevant products in descending order
cb_recom_product
```

```{r,eval = FALSE}
#### CODE NOT IN USE #####
#names(result)

#result_i <- names(result)
#head(result_i)

# create tags for the product
#m1 <- as.matrix(dtm_ti2); dim(m1) # 679 rows, 6392 columns


# Obtain top 5 key words for each product
#tag <- t(apply(m1, 1, FUN = function(x) colnames(m1)[order(-x)[1:5]]))
#tag_unite <- unite(data.frame(tag), "tag", 1:5, sep = " ")

#saveRDS(sim_mat_cos, "./sim_mat_cos.rds")

#recProd = function (simm, prod, k) {
#    found <- sort(simm[, prod], decreasing = TRUE)[2:(k+1)]
#    print(found)
#    cat(paste0("Selected product : <title>", colnames(review_cb_prod2[prod,1])))
#    cat(paste0("Selected product: <title> ", review_cb_prod2[prod, 1]))
#    cat("\nRecommended products:\n")
#    resindex <- names(found)
#    print(resindex)
#    for (i in 1:k) {
#        cat(paste0("\n",i,"-", resindex[i], " <ProductId> ", 
#                   review_cb_prod2[resindex[i], 1], 
#                   "\n<tags> ", 
#                   tag_unite[result_i[i],]))
#    }
#}
################################################################
```

```{r}
#review_final[review_final$ProductId == '0006641040',]
```

#### Hybrid Recommender #####

```{r, eval = FALSE}
#Check for intersect of the recommendations
common <- intersect(cf_recom_product$ProductId, cb_recom_product$ProductId)

common

cf_recom_product[cf_recom_product$ProductId == common,]

cb_recom_product[cb_recom_product$ProductId == common,]
```

Normalize rating from ALS recommender as well as from Content Based Recommender
```{r}
#normalize cf_recom_product and cb_recom_product
cf_recom_product$meanRating = with(cf_recom_product, (meanRating - min(meanRating)) / (max(meanRating) - min(meanRating)))
cb_recom_product$meanRating = with(cb_recom_product, (meanRating - min(meanRating)) / (max(meanRating) - min(meanRating)))
#cf_recom_product
#cb_recom_product
```

Combine the normalized products from each recommender system
```{r}
combi_prod_recom <- rbind(cf_recom_product[,1:2], cb_recom_product[,1:2])

# Combined normalized products recommendation, sorted descending by meanRating
combi_prod_recom <- combi_prod_recom[order(combi_prod_recom$meanRating, decreasing = TRUE),]
#combi_prod_recom

# total number of items
nrow(combi_prod_recom)
```

To ensure some surprise factor in the recommendation, we mixed the recommendation, taking 5 from top, 3 from middle and 2 from bottom.
```{r}
# recommend items from top 5, middle 3 and bottom 2 of the list to introduce a mix
combi_recom_mix <- combi_prod_recom[c(1:5, 
                                      (nrow(combi_prod_recom)/2):(nrow(combi_prod_recom)/2+2),
                                      (nrow(combi_prod_recom)-1): nrow(combi_prod_recom)),
                                   ]
```

The sample customer order:
```{r}
cust_order
```

The resulting recommendation:
```{r}
combi_recom_mix
```

## Conclusion


## Reference
1. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering
R. He, J. McAuley
WWW, 2016
pdf


